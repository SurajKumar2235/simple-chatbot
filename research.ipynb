{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk \n",
    "import string\n",
    "import random\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence present in word 316 no of tokens 5952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/suraj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/suraj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "f=open('chatbot.txt','r',errors='ignore')\n",
    "raw_doc=f.read()\n",
    "raw_doc=raw_doc.lower()\n",
    "nltk.download('punkt') # punct tokenizer\n",
    "nltk.download('wordnet')# dictionary\n",
    "sent_token = nltk.sent_tokenize(raw_doc) #converting doc to list of sentences\n",
    "word_token = nltk.word_tokenize(raw_doc) #converting doc to list of tokenozer\n",
    "\n",
    "\n",
    "print('Sentence present in word',len(sent_token),'no of tokens' ,len(word_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data science combines math and statistics, specialized programming, advanced analytics, artificial intelligence (ai) and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organizationâ€™s data.',\n",
       " 'these insights can be used to guide decision making and strategic planning.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sent_token[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemnt=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(words):\n",
    "    return [lemnt.lemmatize(word) for word in words]\n",
    "\n",
    "\n",
    "remove_punct_dict=dict((ord(punct),None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormilazation(text):\n",
    "    return lemmatize(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### greeting and exting the chatbot flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREET_INPUTS=('hello','hi','greetings','sup',\"what's up\",\"hey\",'hi')\n",
    "\n",
    "GREET_RESPONSE=['hi','hey',\n",
    "                '*nods*',\n",
    "                \"hi there\",\n",
    "                'hello',\n",
    "                'i am glad',\n",
    "                'how are you?',\n",
    "\n",
    "                ]\n",
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREET_INPUTS:\n",
    "            return random.choice(GREET_RESPONSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=LemNormilazation, stop_words='english')\n",
    "Tfidf = vectorizer.fit_transform(sent_token)\n",
    "\n",
    "def response(user_input):\n",
    "    chat_response=''\n",
    "    user_input = LemNormilazation(user_input)\n",
    "    user_input.append(user_input[-1])\n",
    "    tfidf_values = vectorizer.transform(user_input)\n",
    "    cosine_sim_value=cosine_similarity(tfidf_values[-1], Tfidf)\n",
    "    idx = cosine_sim_value.argsort()[0][-2]\n",
    "    flat = cosine_sim_value.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf=flat[-2]\n",
    "\n",
    "    if req_tfidf==0:\n",
    "        chat_response=chat_response+\"I am sorry! I don't understand you\"\n",
    "        return chat_response\n",
    "    else:\n",
    "        chat_response=chat_response+sent_token[idx]\n",
    "\n",
    "        return chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: hi\n",
      "BOT hi\n",
      "BOT: what is data science?\n",
      "BOT: I am sorry! I don't understand you\n",
      "BOT: I am sorry! I don't understand you\n",
      "BOT: what is data science?\n",
      "BOT: what is data science?\n",
      "BOT : bye\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print('BOT: hi')\n",
    "while flag:\n",
    "    user_input=input()\n",
    "    user_input=user_input.lower()\n",
    "    if user_input not in ['bye','tata',\"let's talk later\"]:\n",
    "        if user_input=='thanks' or user_input==\"thank you\" :\n",
    "            flag=False\n",
    "            print('BOT: ok thaks for conversation')\n",
    "\n",
    "        else:\n",
    "            res=greet(user_input)\n",
    "            if (res!= None):\n",
    "                print('BOT' ,res)\n",
    "            else:\n",
    "                sent_token.append(user_input)\n",
    "                word_token.extend(nltk.word_tokenize(user_input))\n",
    "                final_words = list(set(word_token))\n",
    "                print(\"BOT:\", response(user_input))\n",
    "                sent_token.remove(user_input)\n",
    "    else:\n",
    "        flag=False\n",
    "        print('BOT : bye')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
